---
title: "Hotel Review Analysis"
author: "LIL"
date: "September 24, 2018"
output: word_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
	echo = FALSE,
	message = FALSE,
	warning = FALSE
)
```

## Hotel Review

This report is about the analysis of Hotel review :

```{r cars, echo=FALSE, message=FALSE, warning=FALSE}
library("tm")

library(udpipe)

rw=read.csv('HotelRw_csv.csv')

rw_d=data.frame(rw)
library(tm)
myCorpus <- Corpus(VectorSource(rw_d$Review))
removeURL <- function(x) gsub("http[^[:space:]]*", "", x)

# remove URLs

removeURL <- function(x) gsub("http[^[:space:]]*", "", x)



myCorpus <- tm_map(myCorpus, content_transformer(removeURL))


removeNumPunct <- function(x) gsub("[^[:alpha:][:space:]]*", "", x)

myCorpus <- tm_map(myCorpus, content_transformer(removeNumPunct))



myCorpus <- tm_map(myCorpus, content_transformer(tolower))





stopwordlist<-read.csv(file="D:/sales force/tweeter analysis/stopWords.csv",header = FALSE,sep = ",")

myStopwords <- c(setdiff(stopwords('english'), c("r", "big")),"use", "see", "used", "via","https", "amp","will","this","also","first","modi","rahul","jhasanjay")


myStopwords1<-unlist(myStopwords)

stopwordlist<-unlist(stopwordlist)

myCorpus <- tm_map(myCorpus, removeWords, myStopwords1)



myCorpus <- tm_map(myCorpus, removeWords, myStopwords)
myCorpus <- tm_map(myCorpus, removeWords, c("this","will"))
myCorpus <- tm_map(myCorpus, removeWords, c("must","Must","MUST","bjp","Bjp","BJP"))


myCorpus <- tm_map(myCorpus, stripWhitespace)


myCorpusCopy <- myCorpus



library(SnowballC)

tdm <- TermDocumentMatrix(myCorpus,control = list(wordLengths = c(4, Inf)))

library(wordcloud)

m <- as.matrix(tdm)



word.freq <- sort(rowSums(m), decreasing = T)


wordcloud(words = names(word.freq), freq = word.freq,scale=c(4,.5), min.freq = 5,  random.order = F,colors=brewer.pal(6, "Dark2"))

```

## Cooccurance of words while talking about Business


```{r pressure, echo=FALSE, message=FALSE, warning=FALSE}
## Talks about 
library(udpipe)
udmodel <- udpipe_load_model(file = "E://GitHub//HotelReview//english-ud-2.0-170801.udpipe")
x <- udpipe_annotate(udmodel, x = rw_d$Review)
x <- as.data.frame(x)


stats <- keywords_collocation(x = x,  term = "token", group = c("doc_id", "paragraph_id", "sentence_id"),
                             ngram_max = 4)
stats <- cooccurrence(x = subset(x, upos %in% c("NOUN", "ADJ")), term = "lemma", group = c("doc_id","paragraph_id", "sentence_id"))
stats <- cooccurrence(x = x$lemma,   relevant = x$upos %in% c("NOUN", "ADJ"))

stats <- cooccurrence(x = x$lemma,   relevant = x$upos %in% c("NOUN", "ADJ"), skipgram = 2)


library(igraph)
library(ggraph)
library(ggplot2)
wordnetwork <- head(stats, 30)
wordnetwork <- graph_from_data_frame(wordnetwork)
ggraph(wordnetwork, layout = "fr") +
  geom_edge_link(aes(width = cooc, edge_alpha = cooc), edge_colour = "pink") +
  geom_node_text(aes(label = name), col = "darkgreen", size = 3) +
  theme_graph(base_family = "Arial Narrow") +
  theme(legend.position = "none") +
  labs(title = "Cooccurance of words in reviews", subtitle = "Nouns & Adjective")
#####################


```

## Most Occurred Nouns and Adjective in Hotel review
```{r}
i=1
all_view=""
for (i in 1:nrow(rw_d)){
  all_view=paste(all_view, rw_d[i,1])
}


udmodel <- udpipe_load_model(file = "E://GitHub//HotelReview//english-ud-2.0-170801.udpipe")
#udmodel <- udpipe_load_model(file = udmodel$file_model)
x <- udpipe_annotate(udmodel,  x = all_view)
x <- as.data.frame(x)
stats <- subset(x, upos %in% "NOUN")
stats <- subset(x, upos %in% "NOUN")
stats <- txt_freq(x = stats$lemma)

library(lattice)
stats$key <- factor(stats$key, levels = rev(stats$key))
barchart(key ~ freq, data = head(stats, 20), col = "blue", main = "People Most commenting(nouns)", xlab = "Freq")


x <- udpipe_annotate(udmodel,  x = all_view)
x <- as.data.frame(x)
stats <- subset(x, upos %in% "ADJ")

stats <- subset(x, upos %in% "ADJ")
stats <- txt_freq(x = stats$lemma)

library(lattice)
stats$key <- factor(stats$key, levels = rev(stats$key))
barchart(key ~ freq, data = head(stats, 20), col = "blue", main = "People Most commenting(Adjective)", xlab = "Freq")
```

## Reviews commenting about service

Analysis of the reviews while talking about the service 

```{r}

rw_d=data.frame(rw)


i=1
j=1
sel_rw=list()
like=list()
for(i in 1:nrow(rw)){
  if( length(grep("service",toString(tolower(rw[i,1])),value=FALSE))==0)next
  sel_rw[j]=toString(tolower(rw[i,1]))
  like[j]=(rw[i,2])
 
  j=j+1
}

xv <- data.frame(cbind("review" =sel_rw,"like"=like))

myCorpus <- Corpus(VectorSource(xv$review))

removeURL <- function(x) gsub("http[^[:space:]]*", "", x)


removeURL <- function(x) gsub("http[^[:space:]]*", "", x)



myCorpus <- tm_map(myCorpus, content_transformer(removeURL))


removeNumPunct <- function(x) gsub("[^[:alpha:][:space:]]*", "", x)

myCorpus <- tm_map(myCorpus, content_transformer(removeNumPunct))


myCorpus <- tm_map(myCorpus, content_transformer(tolower))



stopwordlist<-read.csv(file="D:/sales force/tweeter analysis/stopWords.csv",header = FALSE,sep = ",")

myStopwords <- c(setdiff(stopwords('english'), c("r", "big","service","food")),"use", "see", "used", "via","https", "amp","will","this","also","first","modi","rahul","jhasanjay","service","food")

#########


myStopwords1<-unlist(myStopwords)

stopwordlist<-unlist(stopwordlist)

myCorpus <- tm_map(myCorpus, removeWords, myStopwords1)


myCorpus <- tm_map(myCorpus, removeWords, myStopwords)

myCorpus <- tm_map(myCorpus, removeWords, c("this","will"))
myCorpus <- tm_map(myCorpus, removeWords, c("must","Must","MUST","bjp","Bjp","BJP"))



myCorpus <- tm_map(myCorpus, stripWhitespace)


myCorpusCopy <- myCorpus


library(SnowballC)

tdm <- TermDocumentMatrix(myCorpus,control = list(wordLengths = c(4, Inf)))

library(wordcloud)

m <- as.matrix(tdm)


word.freq <- sort(rowSums(m), decreasing = T)


wordcloud(words = names(word.freq), freq = word.freq,scale=c(4,.5), min.freq = 2,  random.order = F,colors=brewer.pal(6, "Dark2"))


#####################
##for complete reviews
i=1
rw_txt=""
for (i in 1:nrow(xv)){
  rw_txt=paste(rw_txt, xv[i,1])
}

udmodel <- udpipe_load_model(file = "E://GitHub//HotelReview//english-ud-2.0-170801.udpipe")
#udmodel <- udpipe_load_model(file = udmodel$file_model)
x <- udpipe_annotate(udmodel,  x = rw_txt)
x <- as.data.frame(x)

stats <- subset(x, upos %in% "NOUN")
stats <- txt_freq(x = stats$lemma)

library(lattice)
stats$key <- factor(stats$key, levels = rev(stats$key))
barchart(key ~ freq, data = head(stats, 20), col = "blue", main = "Most occurring nouns", xlab = "Freq")

###Most Occuring Adjective

stats <- subset(x, upos %in% "ADJ")
stats <- txt_freq(x = stats$lemma)

library(lattice)
stats$key <- factor(stats$key, levels = rev(stats$key))
barchart(key ~ freq, data = head(stats, 20), col = "blue", main = "Most occurring Adjective", xlab = "Freq")


###########################################Food#######################



```

## Reviews about Food Analysis

Analysis of reviews while talking about the Food

```{r}
rw_d=data.frame(rw)


i=1
j=1
sel_rw=list()
like=list()
for(i in 1:nrow(rw)){
  if( length(grep("food",toString(tolower(rw[i,1])),value=FALSE))==0)next
  sel_rw[j]=toString(tolower(rw[i,1]))
  like[j]=(rw[i,2])
  #print(toString(res[i,1]))
  j=j+1
}

xv <- data.frame(cbind("review" =sel_rw,"like"=like))

myCorpus <- Corpus(VectorSource(xv$review))

removeURL <- function(x) gsub("http[^[:space:]]*", "", x)



removeURL <- function(x) gsub("http[^[:space:]]*", "", x)



myCorpus <- tm_map(myCorpus, content_transformer(removeURL))



removeNumPunct <- function(x) gsub("[^[:alpha:][:space:]]*", "", x)

myCorpus <- tm_map(myCorpus, content_transformer(removeNumPunct))


myCorpus <- tm_map(myCorpus, content_transformer(tolower))



stopwordlist<-read.csv(file="D:/sales force/tweeter analysis/stopWords.csv",header = FALSE,sep = ",")

myStopwords <- c(setdiff(stopwords('english'), c("r", "big","service","food")),"use", "see", "used", "via","https", "amp","will","this","also","first","modi","rahul","jhasanjay","service","food")

#########

myStopwords1<-unlist(myStopwords)

stopwordlist<-unlist(stopwordlist)

myCorpus <- tm_map(myCorpus, removeWords, myStopwords1)


myCorpus <- tm_map(myCorpus, removeWords, myStopwords)


myCorpus <- tm_map(myCorpus, removeWords, c("this","will"))
myCorpus <- tm_map(myCorpus, removeWords, c("must","Must","MUST","bjp","Bjp","BJP"))



myCorpus <- tm_map(myCorpus, stripWhitespace)



myCorpusCopy <- myCorpus




library(SnowballC)

tdm <- TermDocumentMatrix(myCorpus,control = list(wordLengths = c(4, Inf)))



library(wordcloud)

m <- as.matrix(tdm)



word.freq <- sort(rowSums(m), decreasing = T)


wordcloud(words = names(word.freq), freq = word.freq,scale=c(4,.5), min.freq = 2,  random.order = F,colors=brewer.pal(6, "Dark2"))

#####################

i=1
rw_txt=""
for (i in 1:nrow(xv)){
  rw_txt=paste(rw_txt, xv[i,1])
}

udmodel <- udpipe_load_model(file = "E://GitHub//HotelReview//english-ud-2.0-170801.udpipe")

x <- udpipe_annotate(udmodel,  x = rw_txt)
x <- as.data.frame(x)

stats <- subset(x, upos %in% "NOUN")
stats <- txt_freq(x = stats$lemma)

library(lattice)
stats$key <- factor(stats$key, levels = rev(stats$key))
barchart(key ~ freq, data = head(stats, 20), col = "blue", main = "Most occurring nouns", xlab = "Freq")

###Most Occuring Adjective

stats <- subset(x, upos %in% "ADJ")
stats <- txt_freq(x = stats$lemma)

library(lattice)
stats$key <- factor(stats$key, levels = rev(stats$key))
barchart(key ~ freq, data = head(stats, 20), col = "blue", main = "Most occurring Adjective", xlab = "Freq")
```

## Natural Language Processing
With the data input of 1000 reviews, we have build the model to classify the review into positive or negative.
If we apply same model to sample 200 reviews we will get following result to validate the model.


```{r}
# Natural Language Processing

# Importing the dataset
dataset_original = read.delim('Restaurant_Reviews.tsv', quote = '', stringsAsFactors = FALSE)

# Cleaning the texts
# install.packages('tm')
# install.packages('SnowballC')
library(tm)
library(SnowballC)
corpus = VCorpus(VectorSource(dataset_original$Review))
corpus = tm_map(corpus, content_transformer(tolower))
corpus = tm_map(corpus, removeNumbers)
corpus = tm_map(corpus, removePunctuation)
corpus = tm_map(corpus, removeWords, stopwords())
corpus = tm_map(corpus, stemDocument)
corpus = tm_map(corpus, stripWhitespace)

# Creating the Bag of Words model
dtm = DocumentTermMatrix(corpus)
dtm = removeSparseTerms(dtm, 0.999)
dataset = as.data.frame(as.matrix(dtm))
dataset$Liked = dataset_original$Liked

# Importing the dataset
#dataset = read.csv('Social_Network_Ads.csv')
#dataset = dataset[3:5]

# Encoding the target feature as factor
dataset$Liked = factor(dataset$Liked, levels = c(0, 1))

# Splitting the dataset into the Training set and Test set
# install.packages('caTools')
library(caTools)
set.seed(123)
split = sample.split(dataset$Liked, SplitRatio = 0.8)
training_set = subset(dataset, split == TRUE)
test_set = subset(dataset, split == FALSE)

# Fitting Random Forest Classification to the Training set
# install.packages('randomForest')
library(randomForest)
classifier = randomForest(x = training_set[-692],
                          y = training_set$Liked,
                          ntree = 10)

# Predicting the Test set results
y_pred = predict(classifier, newdata = test_set[-692])

# Making the Confusion Matrix
cm = table(test_set[, 692], y_pred)

print(cm)

library(caret)
print("Sensitivity of Model:")
print(sensitivity(cm))
print("specificity of Model:")
print(specificity(cm))

```

